# -*- coding: utf-8 -*-
"""
Created on Mon Jul 22 06:58:25 2024

@author: azzy
"""

from keys import *
# importing keys
import pandas as pd
import numpy as np #computing multidimensionla arrays
from datetime import datetime
from time import sleep
from binance.client import Client
from binance import *
from binance.enums import *
import os
import pickle
import random
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from scipy.integrate import trapezoid
globalInterval=Client.KLINE_INTERVAL_15MINUTE
########close/open trades###########
def Lsafe(client,Seed,mrgType,lvrg):
    try:
        client.futures_change_leverage(symbol=Seed,leverage=lvrg)
        client.futures_change_margin_type(symbol=Seed,marginType=mrgType)
    except:
        return
    
#Precession
def get_current_datetime_as_string():
    current_datetime = datetime.now()
    return current_datetime.strftime("%Y-%m-%d %H:%M:%S")

def truncate(number, precision):
    factor = 10.0 ** precision
    return int(number * factor) / factor


def LongOrder(client, Seed, precision, numBots, lvrg):
    balance = client.futures_account_balance()
    bal = None
    
    for wc in balance:
        if wc["asset"] == 'USDT':
            bal = float(wc["balance"])
            break

    if bal is None:
        return "No USDT balance found"
    if bal >= 1000:
        bal = 1000
    percent = 0.9 / numBots  # Calculate the percentage of balance to use for each bot

    price = float(client.futures_mark_price(symbol=Seed)["markPrice"])
    maxl = (bal * percent) * lvrg
    maxq = maxl / price
    q = truncate(maxq, precision)

    try:
        result=client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_BUY, quantity=str(q))
        if result['orderId']:
            return str(q)
        else:
            return 'null'
    except:
        return "null"

def ShortOrder(client, Seed, precision, numBots, lvrg):
    balance = client.futures_account_balance()
    bal = None
    
    for wc in balance:
        if wc["asset"] == 'USDT':
            bal = float(wc["balance"])
            break

    if bal is None:
        return "No USDT balance found"

    percent = 0.9 / numBots  # Calculate the percentage of balance to use for each bot

    price = float(client.futures_mark_price(symbol=Seed)["markPrice"])
    maxl = (bal * percent) * lvrg
    maxq = maxl / price
    q = truncate(maxq, precision)

    try:
        result=client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_SELL, quantity=str(q))
        if result['orderId']:
            return str(q)
        else:
            return 'null'
    except:
        return "null"

def closeLong(client, p, Seed):
    try:
        client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_SELL, quantity=p, reduceOnly='true')
        return f"Closed long position with quantity {p}"
    except:
        return 'null'

def closeShort(client, p, Seed):
    try:
        client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_BUY, quantity=p, reduceOnly='true')
        return f"Closed short position with quantity {p}"
    except:
        return 'null'
##########################preceptron###################################
# Define the file path for the Q-table
q_table_file = 'models/q_table.pkl'
# Declare a global variable for cached high movement probability data
cached_high_movement_proba = None
# Load Q-table from memory or initialize if not present

# Function to train the market models incrementally for high movement detection
def train_market(areas, model_file="models/high_movement_model.pkl"):
    # Initialize the classifier for detecting high movement
    mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, warm_start=True)

    # Initialize a scaler for the input data
    scaler = StandardScaler()

    # Create the directory if it does not exist
    os.makedirs(os.path.dirname(model_file), exist_ok=True)

    # Use signed magnitudes (positive or negative areas) to preserve direction
    X = np.array([area for area in areas]).reshape(-1, 1)  # Input features with signed magnitudes
    X_scaled = scaler.fit_transform(X)

    # Dynamically calculate the significant threshold as the mean of the absolute magnitudes
    significant_threshold = np.mean(np.abs(areas))

    # Prepare labels for detection:
    # Label as high movement (1) if area is larger than the threshold and moving away from zero, otherwise low (0)
    y_detection = np.array([1 if abs(area) > significant_threshold else 0 for area in areas])

    # Ensure both X and y are of the same length
    if len(X_scaled) != len(y_detection):
        print("Length mismatch between input and labels.")
        return

    # Check for sufficient data
    if len(X_scaled) < 2:
        print("Not enough data to train.")
        return

    # Check for NaNs
    if np.isnan(X_scaled).any() or np.isnan(y_detection).any():
        print("Data contains NaN values. Skipping this batch.")
        return

    # Load existing model and scaler if they exist
    if os.path.exists(model_file):
        mlp_classifier, scaler = joblib.load(model_file)

    # Ensure all classes are present in the current batch for detection
    unique_classes_detection = np.unique(y_detection)
    if len(unique_classes_detection) == 1:  # Only one class present
        missing_class = 1 - unique_classes_detection[0]  # Find the missing class
        dummy_feature = np.mean(X_scaled)  # Use the mean of scaled features as a dummy feature
        X_scaled = np.insert(X_scaled, 0, [dummy_feature], axis=0)
        y_detection = np.insert(y_detection, 0, missing_class)

    # Incremental fit to the model for detection
    mlp_classifier.partial_fit(X_scaled, y_detection, classes=[0, 1])

    # Save the model
    joblib.dump((mlp_classifier, scaler), model_file)


# Function to detect high movement probability using the trained model
def detect_high_movement(areas, model_file="models/high_movement_model.pkl"):
    # Load the model and scaler
    mlp_classifier, scaler = joblib.load(model_file)

    # Use signed magnitudes for detecting movement
    X = np.array([area for area in areas]).reshape(-1, 1)
    X_scaled = scaler.transform(X)

    # Detection: Probability that the last area indicates high movement away from zero
    last_area = X_scaled[-1].reshape(1, -1)
    detection_proba = mlp_classifier.predict_proba(last_area)[0, 1]

    # Return the probability of high movement
    return detection_proba


def detect_high_movement_cached(areas):
    global cached_high_movement_proba

    # If the cache is empty or the areas change, compute it once
    if cached_high_movement_proba is None:
        cached_high_movement_proba = detect_high_movement(areas)

    return cached_high_movement_proba


##########################################DEEP Q######################################################
# Define the file path for the Q-table
q_table_file = 'models/q_table.pkl'

# Load Q-table from memory or initialize if not present
def load_q_table():
    if os.path.exists(q_table_file):
        with open(q_table_file, 'rb') as f:
            q_table = pickle.load(f)
        print("Loaded Q-table from file.")
    else:
        q_table = {}
        # Initialize only valid state-action pairs
        q_table.update({('not_holding', action): 0 for action in ['entry_long', 'entry_short', 'not_holding']})
        q_table.update({('hold_long', action): 0 for action in ['hold_long', 'exit_long']})
        q_table.update({('hold_short', action): 0 for action in ['hold_short', 'exit_short']})
    return q_table

# Save Q-table to memory
def save_q_table(q_table):
    with open(q_table_file, 'wb') as f:
        pickle.dump(q_table, f)

# Initialize or load the Q-table
q_table = load_q_table()

# Hyperparameters
epsilon = 0.5  # Start with a higher exploration rate to encourage exploration
epsilon_decay = 0.9995  # Use a more gradual decay to reduce exploration slowly
epsilon_min = 0.1  # Higher minimum exploration rate to maintain some exploration
alpha = 0.1  # Learning rate
gamma = 0.8  # Discount factor

# Updated reward function
def reward(state, action, entry_price, exit_price):
    global q_table

    # Calculate profit or loss as a percentage for long and short
    if action == 'exit_long':
        realized_profit_percentage = ((exit_price - entry_price) / entry_price) * 100
        if realized_profit_percentage < 0:
            realized_profit_percentage *= 2  # Double penalty for negative trades
            # Update Q-table for the wrong action (long)
            if (state, action) in q_table:
                old_value = q_table[(state, action)]
                next_state = 'not_holding'
                next_max = max(q_table.get((next_state, a), 0) for a in ['entry_long', 'entry_short', 'not_holding'])
                new_value = old_value + alpha * (realized_profit_percentage + gamma * next_max - old_value)
                q_table[(state, action)] = new_value

            # Reward the opposite action (entry_short) for learning
            if (state, 'entry_short') in q_table:
                q_table[(state, 'entry_short')] += abs(realized_profit_percentage)  # Positive reward for short

    elif action == 'exit_short':
        realized_profit_percentage = ((entry_price - exit_price) / entry_price) * 100
        if realized_profit_percentage < 0:
            realized_profit_percentage *= 2  # Double penalty for negative trades
            # Update Q-table for the wrong action (short)
            if (state, action) in q_table:
                old_value = q_table[(state, action)]
                next_state = 'not_holding'
                next_max = max(q_table.get((next_state, a), 0) for a in ['entry_long', 'entry_short', 'not_holding'])
                new_value = old_value + alpha * (realized_profit_percentage + gamma * next_max - old_value)
                q_table[(state, action)] = new_value

            # Reward the opposite action (entry_long) for learning
            if (state, 'entry_long') in q_table:
                q_table[(state, 'entry_long')] += abs(realized_profit_percentage)

    else:
        return  # No reward update during entry or holding


def get_action(state, area, trend_direction, high_movement_proba):
    global q_table

    # Define valid actions based on the current state
    if state == 'not_holding':
        valid_actions = ['entry_long', 'entry_short', 'not_holding']
    elif state == 'hold_long':
        valid_actions = ['hold_long', 'exit_long']
    elif state == 'hold_short':
        valid_actions = ['hold_short', 'exit_short']
    else:
        valid_actions = ['not_holding']

    # Exploration vs Exploitation
    if random.uniform(0, 1) < epsilon:
        chosen_action = random.choice(valid_actions)  # Random action from valid actions only
        return chosen_action, random.uniform(0, 1)  # Return random confidence during exploration

    # Compute Q-values for valid actions only
    q_values = np.array([q_table.get((state, action), 0) for action in valid_actions])

    # Shift Q-values to be positive if any are negative
    min_q_value = np.min(q_values)
    if min_q_value < 0:
        q_values -= min_q_value  # Shift Q-values to make them non-negative

    # Sum of Q-values for normalization
    q_sum = np.sum(q_values)
    
    # Avoid dividing by zero
    if q_sum == 0:
        return random.choice(valid_actions), 0.5  # Return neutral confidence if no learned Q-values

    # Select the action with the highest Q-value
    chosen_action_index = np.argmax(q_values)
    chosen_action = valid_actions[chosen_action_index]

    # Confidence is the ratio of the chosen Q-value to the sum of all Q-values
    confidence = q_values[chosen_action_index] / q_sum

    # Adjust the confidence value to reflect the probability of winning big (scale it to 0.5-1.0 for entry/exit)
    if chosen_action in ['entry_long', 'entry_short', 'exit_long', 'exit_short']:
        confidence = 0.5 + confidence * 0.5  # Boost confidence for entry/exit actions
    elif chosen_action in ['hold_long', 'hold_short']:
        confidence = confidence * 0.5  # Lower confidence for holding

    return chosen_action, confidence



def RL_train(df):
    global q_table, epsilon, cached_high_movement_proba

    num_episodes = 0
    wins = 0
    losses = 0
    win_rate = 100
    max_episodes = 100
    
    total_profit_percentage = 0  # Initialize total profit percentage

    # Prepare the features for training: normalized areas and trend
    areas, trend_direction = prepare_features(df)
    
    # Cache values upfront to avoid repeated calculations
    close_prices = df['close'].values
    trend_values = trend_direction.values
    area_values = areas.values

    df_len = len(df)

    # Precompute the high movement probability once and store it globally
    cached_high_movement_proba = detect_high_movement(area_values)

    while num_episodes < max_episodes:
        state = 'not_holding'
        entry_price = None

        for i in range(1, df_len):
            current_price = close_prices[i]
            area = area_values[i]
            trend_value = trend_values[i]

            # Reuse cached high movement probability
            high_movement_proba = detect_high_movement_cached(area_values[:i+1])

            # Get the action and confidence based on the current state
            action, confidence = get_action(state, area, trend_value, high_movement_proba)

            # Debugging: print action confidence
            #print(f"Action: {action}, Confidence: {confidence}")

            # Only act if the confidence is greater than 0.5
            if confidence > 0.5:
                if state == 'not_holding' and action in ['entry_long', 'entry_short']:
                    entry_price = current_price
                    state = 'hold_long' if action == 'entry_long' else 'hold_short'

                elif state == 'hold_long' and action == 'exit_long':
                    exit_price = current_price
                    realized_profit_percentage = ((exit_price - entry_price) / entry_price) * 100
                    total_profit_percentage += realized_profit_percentage
                    reward(state, action, entry_price, exit_price)
                    state = 'not_holding'
                    entry_price = None

                elif state == 'hold_short' and action == 'exit_short':
                    exit_price = current_price
                    realized_profit_percentage = ((entry_price - exit_price) / entry_price) * 100
                    total_profit_percentage += realized_profit_percentage
                    reward(state, action, entry_price, exit_price)
                    state = 'not_holding'
                    entry_price = None

        # Force-close any open trades at the end of the dataset
        if state == 'hold_long':
            exit_price = close_prices[-1]
            realized_profit_percentage = ((exit_price - entry_price) / entry_price) * 100
            total_profit_percentage += realized_profit_percentage
            reward(state, 'exit_long', entry_price, exit_price)
            if exit_price > entry_price:
                wins += 1
            else:
                losses += 1

        elif state == 'hold_short':
            exit_price = close_prices[-1]
            realized_profit_percentage = ((entry_price - exit_price) / entry_price) * 100
            total_profit_percentage += realized_profit_percentage
            reward(state, 'exit_short', entry_price, exit_price)
            if exit_price < entry_price:
                wins += 1
            else:
                losses += 1

        num_episodes += 1

        if wins + losses > 0:
            win_rate = (wins / (wins + losses)) * 100

        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

    return win_rate, total_profit_percentage


def run_training(TokenInfo):
    global q_table

    winratecoll = {}

    for Ti, df in TokenInfo.items():
        try:
            win_rate, total_profit = RL_train(df)   
            winratecoll[Ti] = [round(win_rate, 3), round(total_profit, 3)]
        except Exception as e:
            print(f"RL: {str(e)}")
            continue

    save_q_table(q_table)

    return winratecoll

#################### Get token info #########################

def batchCollector(excludedlist,CurrencyType):
    client = Client(api_key, api_secret)
    exInfo=client.futures_exchange_info()
    tokenInf,sinf=FindNewToken(client,exInfo,CurrencyType)
    Ftoken_list=OpenLayer(excludedlist,tokenInf)
    return Ftoken_list,sinf

def FindNewToken(client,exInfo,CurrencyType):
    symInfo={}
    tokenInfo={}
    for symbol in exInfo["symbols"]:
        if symbol["contractType"]=="PERPETUAL" and CurrencyType in symbol["symbol"] and symbol["status"]=="TRADING":
            symInfo[symbol["symbol"]]=symbol["quantityPrecision"]
    x=0
    for key,values in symInfo.items():
        try:
            candles = client.futures_continous_klines(pair=key, interval=globalInterval,ContractType='PERPETUAL')
            df = pd.DataFrame(candles)
            df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
            df['timestart'] = df['timestart'] / 1000
            df['timeend'] = df['timeend'] / 1000
            df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
            tokenInfo[key]=df
            if x==30:
                print("Token Search Started...")
            x+=1           
        except Exception as e:
            #TokenInfo error Removed from Market or not added yet
            print("TokenInfo error Removed from Market or not added yet",key,e)
            continue
    return tokenInfo,symInfo

def OpenLayer(excludedlist, TokenInfo):
    ranked_result = {}

    print("Looking at the tokens")

    # Training Loop
    for Ti, df in TokenInfo.items():
        try:
            areas, trend_direction = prepare_features(df)
            train_market(areas)
        except Exception as e:
            print(f"Looking: {str(e)}")  # Keep this for debugging
            continue

    print("Training on the tokens")
    # Training Loop
    winratecoll = run_training(TokenInfo)
    
    # Prediction Loop
    for Ti, df in TokenInfo.items():
        try:
            if Ti not in excludedlist:
                areas, trend_direction = prepare_features(df)
                area = areas.iloc[-1]  # Get the moving average value
                trend_value = trend_direction.iloc[-1]  # Get the trend direction (1 for uptrend, -1 for downtrend)
                high_movement_proba = detect_high_movement(areas)  # Check the probability for high movement
                action, confidence = get_action('not_holding', area, trend_value, high_movement_proba)
                if confidence>0.5:
                    if action == 'entry_long':
                        ranked_result[Ti] = [confidence, 'FL']
                    if action == 'entry_short':
                        ranked_result[Ti] = [confidence, 'FS']
        except Exception as e:
            print(f"predictloop error: {str(e)}")  # Keep this for debugging
            continue
    
    # Sort the keys based on confidence (1st element in the list) in descending order
    sorted_keys = sorted(ranked_result.keys(), key=lambda x: ranked_result[x][0], reverse=True)
    
    # Create the sorted dictionary based on the sorted keys
    sorted_ranked_result = {key: ranked_result[key] for key in sorted_keys}
    
    return sorted_ranked_result


#################### Get token info #########################

############# Indicators #############
# Handle NaNs helper function
def handle_nans(series, method='drop'):
    if method == 'drop':
        return series.dropna()
    elif method == 'zero_fill':
        return series.fillna(0)
    elif method == 'mean_fill':
        return series.fillna(series.mean())
    else:
        raise ValueError("Invalid method specified for NaN handling.")

# Average True Range (ATR)
def calculate_atr(df, period=10):
    """Calculate Average True Range (ATR) for a given period."""
    df = df[['high', 'low', 'close']].dropna()  # Ensure no NaN values in required columns

    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    
    # Use np.maximum to combine values for true range
    tr = np.maximum(high_low, np.maximum(high_close, low_close))
    
    # Efficient rolling mean with a minimum period to handle missing data
    atr = tr.rolling(window=period, min_periods=1).mean()
    
    return atr

# SuperTrend Indicator
def superTrend(df, period=10, multiplier=3):
    """Calculate Supertrend indicator and return direction (1 for uptrend, -1 for downtrend)."""
    atr = calculate_atr(df, period)
    
    hl2 = (df['high'] + df['low']) / 2
    upper_band = hl2 + (multiplier * atr)
    lower_band = hl2 - (multiplier * atr)
    
    # Initialize Supertrend and direction columns
    supertrend = pd.Series(index=df.index)
    direction = pd.Series(index=df.index, dtype=int)
    
    # Initialize first values
    supertrend.iloc[0] = lower_band.iloc[0]
    direction.iloc[0] = 1
    
    close_prices = df['close']
    
    for i in range(1, len(df)):
        if direction.iloc[i-1] == 1:
            supertrend.iloc[i] = max(lower_band.iloc[i], supertrend.iloc[i-1]) if close_prices.iloc[i] > lower_band.iloc[i] else upper_band.iloc[i]
            direction.iloc[i] = 1 if close_prices.iloc[i] > supertrend.iloc[i] else -1
        else:
            supertrend.iloc[i] = min(upper_band.iloc[i], supertrend.iloc[i-1]) if close_prices.iloc[i] < upper_band.iloc[i] else lower_band.iloc[i]
            direction.iloc[i] = -1 if close_prices.iloc[i] < supertrend.iloc[i] else 1
    
    return direction

# Moving Average
def moving_average(series, window=14):
    return series.rolling(window=window, min_periods=1).mean()

# RSI Normalized
def RSI_Normalized(df, period=14):
    delta = df['close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=period, min_periods=1).mean()
    avg_loss = loss.rolling(window=period, min_periods=1).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))

    rsi_normalized = (rsi - 50) / 50
    rsi_normalized = rsi_normalized.clip(-1, 1)

    rsi_normalized = handle_nans(rsi_normalized, method='mean_fill')
    
    return rsi_normalized

# MACD Normalized
def MACD_Normalized(df, short_period=12, long_period=26, signal_period=9):
    df = df[['close']].dropna()  # Ensure no NaNs in 'close'
    
    short_ema = df['close'].ewm(span=short_period, adjust=False).mean()
    long_ema = df['close'].ewm(span=long_period, adjust=False).mean()
    
    macd_line = short_ema - long_ema
    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()
    macd_histogram = macd_line - signal_line
    
    macd_min = macd_histogram.min()
    macd_max = macd_histogram.max()
    macd_normalized = 2 * (macd_histogram - macd_min) / (macd_max - macd_min) - 1
    macd_normalized = macd_normalized.clip(-1, 1)

    macd_normalized = handle_nans(macd_normalized, method='mean_fill')
    
    return macd_normalized

# CMF Normalized
def CMF_Normalized(df, period=20):
    mf = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low'])
    mfv = mf * df['volume']
    cmf = mfv.rolling(period).sum() / df['volume'].rolling(period).sum()

    cmf_min = cmf.min()
    cmf_max = cmf.max()
    cmf_normalized = 2 * (cmf - cmf_min) / (cmf_max - cmf_min) - 1
    cmf_normalized = cmf_normalized.clip(-1, 1)

    cmf_normalized = handle_nans(cmf_normalized, method='mean_fill')
    
    return cmf_normalized

# Calculate Moving Average Areas using trapezoidal integration
def calculate_moving_avg_areas(moving_avg_data, window_size=5):
    """
    Calculate areas based on moving averages using a rolling window and trapezoidal integration.
    """
    diff_data = moving_avg_data.diff()

    # Safe trapezoidal calculation
    def safe_trapezoid(x):
        if len(x) >= 2:  # Only apply trapezoid if we have enough data points
            return trapezoid(x, dx=1)
        return 0  # Return 0 for insufficient points

    areas = diff_data.rolling(window=window_size).apply(lambda x: safe_trapezoid(x), raw=True)
    areas = handle_nans(areas, method='mean_fill')
    
    return areas

# Prepare features by combining indicators and areas
def prepare_features(df):
    
    # Calculate trend and indicators
    Trend = superTrend(df)
    RSI = RSI_Normalized(df)
    MACD = MACD_Normalized(df)
    CMF = CMF_Normalized(df)
    
    # Combine normalized indicators
    data = (RSI + MACD + CMF) / 3
    
    # Calculate moving average areas from combined indicators
    areas = calculate_moving_avg_areas(data) * 100  # Multiplying for better scaling
    areas = moving_average(areas, window=20)  # Apply smoothing on the areas

    # Calculate the rate of change (velocity) of the area to track sudden shifts
    area_velocity = np.gradient(areas)  # This gives the rate of change of the areas

    # Normalize the close price between -1 and 1
    scaler = MinMaxScaler(feature_range=(-1, 1))  # Set range from -1 to 1
    normalized_close_price = scaler.fit_transform(df['close'].values.reshape(-1, 1)).flatten()  # Normalize
    
    # Combine the normalized close price with the areas and velocity (by averaging)
    combined_features = (areas + normalized_close_price + area_velocity) / 3  # Average the areas, close price, and velocity

    return combined_features, Trend  # Return the combined features and trend

########crazy NewWave############
def newwave_Trader(CurrencyType,BotLimit,lvrg,mode):
    global globalInterval
    client = Client(api_key, api_secret)
    mrgType="ISOLATED"
    TradeLog={}
    while True:
        keys_to_remove = []
        if len(TradeLog)<BotLimit:
            balance = client.futures_account_balance()
            bal = None
            for wc in balance:
                if wc["asset"] == 'USDT':
                    bal = float(wc["balance"])
            if bal > initial_amt:
                amnt = bal - initial_amt
                client.futures_account_transfer(asset='USDT', amount=amnt, Type=2)
                print("Profit Transfer:", amnt)
            if bal > 100:
                BotLimit=4
            if bal > 500:
                BotLimit=8
            TokenList,tinfo = batchCollector(excludedlist,CurrencyType)
            print('tokens found:',len(TokenList))
            
            for Ti,ttype in TokenList.items():
                if Ti not in TradeLog: 
                    Lsafe(client, Ti, mrgType, lvrg)
                    candles = client.futures_continous_klines(pair=Ti, interval=globalInterval,ContractType='PERPETUAL')
                    df = pd.DataFrame(candles)
                    df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
                    df['timestart'] = df['timestart'] / 1000
                    df['timeend'] = df['timeend'] / 1000
                    df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
        
                    if ttype[1]=="FL" :
                        opprice = df['close'].iloc[-1]
                        if mode=='R':
                            p=LongOrder(client,Ti,tinfo[Ti],BotLimit,lvrg)
                            if p!="null" and p!='':    
                                print("added:",Ti,ttype)
                                TradeLog[Ti] = [ttype[1],opprice,p]
                                sleep(5)
                        else:
                            print("added:",Ti,ttype)
                            TradeLog[Ti] = [ttype[1],opprice,0]          
                    if ttype[1]=="FS":
                        opprice = df['close'].iloc[-1]
                        if mode=='R':
                            p=ShortOrder(client,Ti,tinfo[Ti],BotLimit,lvrg)
                            if p!="null" and p!='':
                                print("added:",Ti,ttype)
                                TradeLog[Ti] = [ttype[1],opprice,p]
                                sleep(5)
                        else:
                            print("added:",Ti,ttype)
                            TradeLog[Ti] = [ttype[1],opprice,0]
                            
                    if len(TradeLog)>=BotLimit:
                        break
                sleep(5)
                
        if len(TradeLog)>=BotLimit: #kahbib helped me add this line
            for Ti, Token_info in TradeLog.items():
                status = Token_info[0]
                entry_price = Token_info[1]
                amount = Token_info[2]
        
                # Fetch latest market data
                candles = client.futures_continous_klines(pair=Ti, interval=globalInterval, ContractType='PERPETUAL')
                df = pd.DataFrame(candles)
                df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume',
                              'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
                df['timestart'] = df['timestart'] / 1000
                df['timeend'] = df['timeend'] / 1000
                df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume',
                    'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume',
                                                           'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
                exit_price = df['close'].iloc[-1]
                areas, trend_direction = prepare_features(df)
                area = areas.iloc[-1]  # Get the moving average value
                trend_value = trend_direction.iloc[-1]  # Get the trend direction (1 for uptrend, -1 for downtrend)
                # Get high movement probability based on the areas (moving averages)
                high_movement_proba = detect_high_movement(areas)  # Check the probability for high movement
                # Decision making based on the current state and calculated features
                if status == 'FS':
                    action,confidence = get_action('hold_short',area, trend_value, high_movement_proba)
                    if action == 'exit_short' and confidence>0.5:
                        if entry_price > exit_price:
                            if mode == 'R':
                                closeShort(client, amount, Ti)
                            print("trade closed : ", Ti, entry_price, exit_price, 'W')
                            keys_to_remove.append(Ti)
                        else:
                            if mode == 'R':
                                closeShort(client, amount, Ti)
                            print("trade closed : ", Ti, entry_price, exit_price, 'L')
                            keys_to_remove.append(Ti)
                        reward('hold_short', action, entry_price, exit_price)
                        save_q_table(q_table)
        
                if status == 'FL':
                    action,confidence  = get_action('hold_long', area, trend_value, high_movement_proba)
                    if action == 'exit_long' and confidence>0.5:
                        if entry_price < exit_price:
                            if mode == 'R':
                                closeLong(client, amount, Ti)
                            print("trade closed : ", Ti, entry_price, exit_price, 'W')
                            keys_to_remove.append(Ti)
                        else:
                            if mode == 'R':
                                closeLong(client, amount, Ti)
                            print("trade closed : ", Ti, entry_price, exit_price, 'L')
                            keys_to_remove.append(Ti)
                        reward('hold_long', action, entry_price, exit_price)
                        save_q_table(q_table)
        
            # Remove all elements corresponding to the keys in the list
            for key in keys_to_remove:
                TradeLog.pop(key, None)
       
        sleep(900)
   
       
BotLimit=3
mode='R'
print("Booting up... ")
excludedlist=['BTCUSDT','BTCDOMUSDT','USDCUSDT','ETHUSDT','XEMUSDT']
CurrencyType="USDT"
lvrg=2
initial_amt=1000
newwave_Trader(CurrencyType,BotLimit,lvrg,mode)
